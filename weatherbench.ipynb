{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usMyjJA8-p7r",
    "tags": []
   },
   "source": [
    "# Simple example of a WeatherBench model\n",
    "\n",
    "In this notebook we will build a simple neural network on the WeatherBench dataset. \n",
    "\n",
    "The aim of this example is to predict the geopotential at the 500hPa pressure level in the atmosphere and compare your solution with the benchmark dataset. This variable is important for identifying weather systems such as cyclones and anticyclones.\n",
    "\n",
    "With the data we can make a forecast for any number of days ahead, but in this exercise we focus on forecasting the geopotential five days ahead.\n",
    "\n",
    "We evaluate our model using the Root Mean Squared Error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8F1nu9mGFz_T"
   },
   "source": [
    "## Import packages\n",
    "\n",
    "Below is the very first code block to \"run\". This will import software packages needed to run the exercise. They include [numpy](https://numpy.org/doc/stable/) and [xarray](https://docs.xarray.dev/en/stable/), which are used for handling multidimentional arrays of data. They also include [tensorflow](https://www.tensorflow.org/) which is a popular Machine Learning package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CsBr8fpZhDnu"
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "## Load in the necessary python packages to train a neural network\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf #tensorflow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2TJnQs1-_s9"
   },
   "source": [
    "## Download data\n",
    "\n",
    "We need to download the WeatherBench data from the public repository. We will only download the 500hPa geopotential data at 5.625 degrees, because training on the whole benchmark dataset using colab would take a very long time and is thus beyond the scope of this exercise. \n",
    "\n",
    "#### Note this download may take over 20 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-5lP1dbj0zjF",
    "outputId": "513f0cce-43f2-4773-b586-ab88b3b8fdfb"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    !wget https://get.ecmwf.int/repository/mooc-machine-learning-weather-climate/geopotential_500_5.625deg.zip\n",
    "except:\n",
    "    !wget --no-check-certificate \"https://dataserv.ub.tum.de/s/m1524895/download?path=%2F5.625deg%2Fgeopotential_500&files=geopotential_500_5.625deg.zip\" -O geopotential_500_5.625deg.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k5Dys1d6n44X",
    "outputId": "713bfc31-1f60-427d-9072-389649d93438"
   },
   "outputs": [],
   "source": [
    "# Unzip the data\n",
    "!unzip geopotential_500_5.625deg.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGhPyQz__RbC"
   },
   "source": [
    "Next we open the dataset using xarray. We will also, for the sake of speed and simplicity, load data only every 12 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "id": "qF8xFfBfhCZG",
    "outputId": "d62b7bb7-d7f4-4572-8bcc-245b97fee2cc"
   },
   "outputs": [],
   "source": [
    "z500 = xr.open_mfdataset('geopotential_500*.nc', combine='by_coords').isel(time=slice(None, None, 1))\n",
    "z500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "id": "u_11kVFcjYD3",
    "outputId": "3e1a80ea-2a29-4897-f662-e6453c4ce7e3"
   },
   "outputs": [],
   "source": [
    "# Now we can load the data into memory to speed up accessing data. This should take <30s\n",
    "z500.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "nHD8mLNjhKcq",
    "outputId": "5dbcd8de-f396-4102-b97b-72f2c6a4570f"
   },
   "outputs": [],
   "source": [
    "# Plot the geopotential at an example date-time\n",
    "z500.z.isel(time=0).plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOrv4B8yh164"
   },
   "source": [
    "## Compute baselines\n",
    "\n",
    "Before training an ML model it is important to have some baselines. Here, we will compute a climatology baseline. For this, we will use the training time period (1979 to 2015) and compute a climatology for each day of the year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O3jl5Gd1jl5Y"
   },
   "outputs": [],
   "source": [
    "# training dataset selection\n",
    "train_years = slice('1979', '2015')\n",
    "# validation dataset selection (this dataset helps with overfitting)\n",
    "valid_years = slice('2016', '2016')\n",
    "# test dataset selection\n",
    "test_years = slice('2017', '2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4uUbB6mDiE9x"
   },
   "outputs": [],
   "source": [
    "def computed_weighted_rmse(fc, gt):\n",
    "  \"\"\"Error metric to compute the area averaged RMSE.\"\"\"\n",
    "  error = fc - gt\n",
    "  weights_lat = np.cos(np.deg2rad(error.lat))\n",
    "  weights_lat /= weights_lat.mean()\n",
    "  rmse = np.sqrt(((error)**2 * weights_lat).mean(('time', 'lat', 'lon')))\n",
    "  return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PB6zoeIZZO-2"
   },
   "source": [
    "Here we consider two baselines: the persistence and the climatology. The persistence is calculated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "id": "7QAqzAykZW-9",
    "outputId": "5decc718-56b7-4d9c-e826-d5c1790f2a89"
   },
   "outputs": [],
   "source": [
    "# pick the forecast lead time\n",
    "lead_time_steps = 10 # 5 day forecast because considering midday and midnight\n",
    "\n",
    "# compute persistent forecast \n",
    "persistence_fc = z500.sel(time=test_years).isel(time=slice(0, -lead_time_steps))\n",
    "persistence_fc['time'] = persistence_fc.time + np.timedelta64(5, 'D').astype('timedelta64[ns]')\n",
    "\n",
    "# target data\n",
    "target = z500.sel(time=test_years)['z']\n",
    "# compute RMSE\n",
    "computed_weighted_rmse(persistence_fc, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKeHi02PATZr"
   },
   "source": [
    "The climatology is calculated for each day of year from the training time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "id": "Dip-vm9Ci7AK",
    "outputId": "f4baa6e5-1a77-42d3-dd73-1717df9693b7"
   },
   "outputs": [],
   "source": [
    "clim = z500.sel(time=train_years).groupby('time.dayofyear').mean()\n",
    "# compute RMSE\n",
    "computed_weighted_rmse(clim.sel(dayofyear=z500.sel(time=test_years).time.dt.dayofyear), z500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kenbc82knEU"
   },
   "source": [
    "## Train a simple CNN\n",
    "\n",
    "Now we can train a simple convolutional neural network. We will use Keras for this. First though we need to prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_svOFJvkpdL"
   },
   "outputs": [],
   "source": [
    "def get_train_valid_test_dataset(lead_steps, z500_dataset):\n",
    "  # Split train, valid and test dataset\n",
    "  train_data = z500_dataset.sel(time=train_years)\n",
    "  valid_data = z500_dataset.sel(time=valid_years)\n",
    "  test_data = z500_dataset.sel(time=test_years)\n",
    "\n",
    "  # Normalize the data using the mean and standard deviation of the training data\n",
    "  mean = train_data.mean()\n",
    "  std = train_data.std()\n",
    "\n",
    "  train_data = (train_data - mean) / std\n",
    "  valid_data = (valid_data - mean) / std\n",
    "  test_data = (test_data - mean) / std\n",
    "\n",
    "  mean = mean['z'].values # extract numerical value from xarray Dataset\n",
    "  std = std['z'].values # extract numerical value from xarray Dataset\n",
    "\n",
    "  # Create inputs and outputs that are shifted by lead_steps\n",
    "  X_train = train_data.z.isel(time=slice(None, -lead_steps)).values[..., None]\n",
    "  Y_train = train_data.z.isel(time=slice(lead_steps, None)).values[..., None]\n",
    "  X_valid = valid_data.z.isel(time=slice(None, -lead_steps)).values[..., None]\n",
    "  Y_valid = valid_data.z.isel(time=slice(lead_steps, None)).values[..., None]  \n",
    "  X_test = test_data.z.isel(time=slice(None, -lead_steps)).values[..., None]\n",
    "  Y_test = test_data.z.isel(time=slice(lead_steps, None)).values[..., None]\n",
    "  return X_train, Y_train, X_valid, Y_valid, X_test, Y_test, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xUXE9Z4jmkzG",
    "outputId": "96d568cf-92d8-401c-efe8-e8f569d3b392"
   },
   "outputs": [],
   "source": [
    "X_train, Y_train, X_valid, Y_valid, X_test, Y_test, mean, std = get_train_valid_test_dataset(lead_time_steps, z500)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(Y_valid.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "va694KRDChmV"
   },
   "source": [
    "### Build model\n",
    "\n",
    "Next we will build the model using Keras. There are many guides for Keras out there, for example [this](https://keras.io/examples/vision/mnist_convnet/). Here we will build a convolutional neural network which we briefly discussed at the end of the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XOiL3zzynV96"
   },
   "outputs": [],
   "source": [
    "# CNN\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(32, 5, padding='same'),   # 32 channels with a 5x5 convolution\n",
    "    keras.layers.ELU(),  # Slightly smoother alternative to ReLU\n",
    "    keras.layers.Conv2D(32, 5, padding='same'),   # Same padding keeps the size identical.\n",
    "    keras.layers.ELU(),\n",
    "    keras.layers.Conv2D(1, 5, padding='same'),\n",
    "    # No activation since we are solving a regression problem\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dj1bTrroEtz_"
   },
   "source": [
    "Next we need to build the model using an example batch and compile it. As an optimizer we will use the standard Adam optimizer combined with a Mean Squared Error Loss. Details on the Adam optimizer can be found here: Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SPR_Jxn4qFgb"
   },
   "outputs": [],
   "source": [
    "model.build(X_train[:32].shape)\n",
    "model.compile(keras.optimizers.Adam(1e-4), 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M9SGCnl3o5ZQ",
    "outputId": "30558f14-b69b-4c3c-f308-ed6a220c31b2"
   },
   "outputs": [],
   "source": [
    "# With .summary() we can check the shape of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "iSw_xTEwp9lc",
    "outputId": "0ccd6d52-77d2-4a87-8504-f16699adbc9e"
   },
   "outputs": [],
   "source": [
    "# Finally we can fit the model.\n",
    "# For each epoch, the entire training dataset has passed through the neural network exactly once\n",
    "# Each epoch should take about 10s\n",
    "\n",
    "model.fit(X_train, Y_train, batch_size=32, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FqMibmtJ1zh3",
    "outputId": "eb14a4e1-d07a-4334-e5b3-300dfaae8826"
   },
   "outputs": [],
   "source": [
    "# Convert predictions backto xarray\n",
    "pred_test = X_test[:, :, :, 0].copy()\n",
    "pred_test[:] = model.predict(X_test).squeeze()   # To remove channel dimension which is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmBSJMtKFKTw"
   },
   "source": [
    "For network training we normalized the data by subtracting the mean and dividing by the standard deviation. To evaluate the predictions, we now need to un-normalize the data using the mean and standard deviation we used to normalize it. Then we can compute the RMSE of the predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177
    },
    "id": "FLFR7JklyVJz",
    "outputId": "2ccacd45-5a27-4749-ca4a-5806707f76f5"
   },
   "outputs": [],
   "source": [
    "# Unnormalize\n",
    "pred_test = pred_test * std + mean\n",
    "# compute RMSE\n",
    "computed_weighted_rmse(pred_test, target.isel(time=slice(lead_time_steps, None)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xq1O6BaX15YG"
   },
   "source": [
    "How does the skill compare to the climatology?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pAFm8Ez839q3"
   },
   "source": [
    "It is always important to visualize your models predictions. Here we take a time from the test period and visualize the ground truth, the climatology and the neural networks' predictions. What do you notice? How about if you try a different time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 259
    },
    "id": "vN9pX_IV37VW",
    "outputId": "9edd77f4-ff98-4662-9211-544c8f8d5ee9"
   },
   "outputs": [],
   "source": [
    "# Note first you need to modify your predictions so they are an xarray instead of a numpy array\n",
    "# This way you can access the latitude, longitude and time for each point in the array\n",
    "\n",
    "# We do this by taking a copy of the original z500 object which has the correct time, \n",
    "# latitude and longitude, and replacing the data in this array with the predictions\n",
    "pred_xarray = z500.z.sel(time=test_years).isel(time=slice(lead_time_steps, None)).copy()\n",
    "pred_xarray.data = pred_test\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "t = xr.DataArray(np.datetime64('2017-10-01T00'))\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 3))\n",
    "\n",
    "z500.z.sel(time=t).plot(ax=ax1)\n",
    "ax1.set_title('Ground truth')\n",
    "\n",
    "clim.z.sel(dayofyear=t.dt.dayofyear).plot(ax=ax2)\n",
    "ax2.set_title('Climatology')\n",
    "\n",
    "pred_xarray.sel(time=t).plot(ax=ax3)\n",
    "ax3.set_title('Prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZVsNZRyFz_Z"
   },
   "source": [
    "The prediction is a lot smoother compared to the ground truth, almost as smooth as climatology. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NA8xTj3HOj4"
   },
   "source": [
    "## Ideas and Additional Exercises\n",
    "\n",
    "The model we built isn't particularly good. How could we make it better? Here are some ideas to try.\n",
    "\n",
    "- Make the neural net bigger/smaller? \n",
    "\n",
    "  We provide the following function for you to easily construct a neural network with more layers:\n",
    "\n",
    "```\n",
    "def build_model_cnn(no_of_layers):\n",
    "    \"\"\"Fully convolutional network\"\"\"\n",
    "    x = input = keras.layers.Input(shape=(32, 64, 1))\n",
    "    x = Conv2D(64, 5, padding = 'same')(x)    \n",
    "    for i in range(no_of_layers):\n",
    "        x = Conv2D(64, 5, padding = 'same')(x)\n",
    "        x = tf.keras.layers.Activation('ReLU')(x)\n",
    "    output = Conv2D(1, 5, padding = 'same')(x)\n",
    "    model = tf.keras.models.Model(input, output)\n",
    "    model.compile(keras.optimizers.Adam(1e-4), 'mse')\n",
    "    print(model.summary())\n",
    "    return model\n",
    "```\n",
    "\n",
    "- Add more variables, for example 850hPa temperature, which is available, check: https://github.com/pangeo-data/WeatherBench\n",
    "- Currently, the convolutions do not wrap around the Earth. You could implement periodic convolutions, see [here](https://github.com/pangeo-data/WeatherBench/blob/master/src/train_nn.py#L102). "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
